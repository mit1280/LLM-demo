{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QRZy6z9Bi3Oy"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q protobuf accelerate optimum auto-gptq gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Quantized Zephyr model from Hugginface"
      ],
      "metadata": {
        "id": "qlNrpr0NthmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-alpha-GPTQ\" # \"TheBloke/Mistral-7B-OpenOrca-GPTQ\"\n",
        "# To use a different branch, change revision\n",
        "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                            #  torch_dtype=torch.bfloat16,\n",
        "                                             revision=\"main\")"
      ],
      "metadata": {
        "id": "zBkJarHvi8LK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loding tokenizer from the parent repo\n",
        "  -- Quantized model repo doesn't has chat template"
      ],
      "metadata": {
        "id": "w1fwgKIGtyPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")"
      ],
      "metadata": {
        "id": "nUyFNtgdp70d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb053f3-3ef5-4ca0-f8c1-c8695e158e41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defination of chat template in Jinja format"
      ],
      "metadata": {
        "id": "Vr1ltzbbt5r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.chat_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "PnNzGSGKkTXy",
        "outputId": "d999cd7f-cc05-49a2-aa61-30196c826441"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of Model inference using the chat template"
      ],
      "metadata": {
        "id": "jDIQICBUt-4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    # Information about SiftMed: Product About Sift Uni Contact Request Demo Request Demo We Want To Transform Medical Reviews Our Mission: It all started with a bankerâ\\x80\\x99s box of files, a kitchen table and one overwhelmed medical file reviewer. Our Co-Founders had a vision - develop a way to easily sift through medical records to find quintessential facts.No one should struggle to find facts in medical data. Ever.â\\x80\\x8dOur Mission - To revolutionize the way the world reviews medical records.Â\\xa0No matter your role in a personal injury case, reviewing medical files for the facts can be a demanding, yet critical, process. Our goal is to help law firms and legal departments automate and streamline their medical record review processes, saving them time, increasing efficiency, and improving report accuracy, leading to better outcomes for clients and greater success for the firm. It all started with a bankerâ\\x80\\x99s box of files, a kitchen table and one overwhelmed medical file reviewer. Our Co-Founders had a vision - develop a way to easily sift through medical records to find quintessential facts.No one should struggle to find facts in medical data. Ever. â\\x80\\x8dOur Mission - To revolutionize the way the world reviews medical records.Â\\xa0 No matter your role in a personal injury case, reviewing medical files for the facts can be a demanding, yet critical, process. Our goal is to help law firms and legal departments automate and streamline their medical record review processes, saving them time, increasing efficiency, and improving report accuracy, leading to better outcomes for clients and greater success for the firm. Find Out More SiftMed is built with proprietary technology SiftMed is dedicated to developing and using the most advanced technology available to help improve your case preparation process. Simply upload the medical records you need to review and let our technology guide you. Natural Language Processing &Â\\xa0ML Automates the organization and categorization of records. We take large medical files, allow for them to be chronologically sorted, create categories, and resolve duplicates. Optical Character Recognition Instantly makes your medical records searchable and allows you to easily copy and paste key pieces of information from PDF image files, including handwritten text. Artificial Intelligence Our team is developing Â\\xa0the groundwork for a predictive analytics model that will highlight pertinent diagnosises and critical information in a file. Our Founders Our TeamFounded in 2020, SiftMed has grown significantly! The team is now a diverse group of innovative leaders, data scientists, software engineers, and medical experts.\n",
        "    {\"role\": \"user\", \"content\": \"You are an helpful Assistant, called RIO. Interact cheerfully, humorously, and friendly.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Hello, I will try my best to assist you today\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hi RIO\"}\n",
        "]\n",
        "device = \"cuda\"\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors='pt')\n",
        "\n",
        "model_inputs = encodeds.to(device)\n",
        "model.to(device)\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQXNUyV9o7Mo",
        "outputId": "ff314b99-0a62-40ab-c69c-33cc2b1ddb80"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using sep_token, but it is not set yet.\n",
            "Using cls_token, but it is not set yet.\n",
            "Using mask_token, but it is not set yet.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "You are an helpful Assistant, called RIO. Interact cheerfully, humorously, and friendly.</s> \n",
            "<|assistant|>\n",
            "Hello, I will try my best to assist you today</s> \n",
            "<|user|>\n",
            "Hi RIO</s> \n",
            "<|assistant|>\n",
            "Hi there! How can I assist you today? I'm just a friendly RIO waiting to help you out!</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Demo\n",
        "\n",
        "This demo can help to create Chat interface using which we can talk with LLM and understand it's working. How it's responding to input text. Some of the LLM's hyperparameter are on the screen so you can change it and see how model is reacting to it.\n",
        "\n",
        "1. Chat-Bot:\n",
        "  - This bot is kind of normal bot which respond to user based on user input\n",
        "  - Prompt format is create based on Huggingface Chat template\n",
        "2. Instruction Chat-Bot\n",
        "  - This bot is added to check LLM performace\n",
        "  - This bot react based on Instruction statement\n",
        "  - Very basic prompt was added but it's quite powerful.\n",
        "  - Model response is highly dependnt on the Custom instruction statement\n",
        "\n",
        "\n",
        "  Note: This is the best small model(7B) as of Oct 15, 2023.\n"
      ],
      "metadata": {
        "id": "wrowi8YwFxQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import gradio as gr\n",
        "import gradio as gr\n",
        "import json\n",
        "import io\n",
        "def variable_args_function(text, **kwargs):\n",
        "    for key, value in kwargs.items():\n",
        "        text = text.replace(f\"##{key}\", str(value))\n",
        "\n",
        "    return(text)\n",
        "\n",
        "chat_prompt = \"\"\"<|system|>\\nYou are an helpful Assistant. Interact cheerfully, humorously, and friendly. Make your response to the point and response must be less than 200 tokens</s>\"\"\"\n",
        "\n",
        "custom_chat_prompt = \"\"\"###Instruction:\\n##instruction</s>###Statement:\\n##statement</s>\\n###Response:\\n\"\"\"\n",
        "\n",
        "model_prompts = ['Chat BOT', 'Custom instructions - Chat BOT']\n",
        "\n",
        "def gen_user_assistant_chat(user_list, bot_list):\n",
        "    prompt = \"\"\n",
        "    for i, j in enumerate(user_list):\n",
        "      prompt = prompt + \"<|user|>\\n\" + j +\"</s>\" + \"<|assistant|>\\n\"\n",
        "      try:\n",
        "        prompt = prompt + bot_list[i] + \"</s>\"\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "    if len(prompt) > 1000:\n",
        "      prompt = prompt[500:]\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def format_prompt_gen(chat_prompt, user_list, bot_list):\n",
        "    prompt = gen_user_assistant_chat(user_list, bot_list)\n",
        "    prompt = chat_prompt + prompt\n",
        "    return prompt\n",
        "\n",
        "def format_custom_prompt(custom_prompt, instruction, statement):\n",
        "    return variable_args_function(custom_chat_prompt, instruction=instruction, statement=statement)\n",
        "\n",
        "def reset(z):\n",
        "    return [], []\n",
        "\n",
        "def create_gradio_app(\n",
        "    model,concurrency_count=4, share=True\n",
        "):\n",
        "  prompt_list = []\n",
        "  bot_list = []\n",
        "  def generate(\n",
        "          prompt,\n",
        "          history,\n",
        "          selected_task,\n",
        "          custom_instruct,\n",
        "          top_p_slider, temperature_slider, max_new_tokens_textbox\n",
        "      ):\n",
        "          prompt_list.append(prompt)\n",
        "          if selected_task == 'Custom instructions - Chat BOT':\n",
        "              # if custom_instruct == '':\n",
        "              #     custom_prompt = chat_prompt\n",
        "              formatted_prompt = format_custom_prompt(custom_chat_prompt, custom_instruct, prompt)\n",
        "          else:\n",
        "              #General\n",
        "              formatted_prompt = format_prompt_gen(chat_prompt, prompt_list, bot_list)\n",
        "\n",
        "\n",
        "          input_ids = tokenizer(formatted_prompt, return_tensors='pt').input_ids.cuda()\n",
        "          output = model.generate(inputs=input_ids, temperature=temperature_slider, do_sample=True, top_p=top_p_slider, top_k=40, max_new_tokens=int(max_new_tokens_textbox))\n",
        "\n",
        "          output_text = tokenizer.decode(output[0])\n",
        "          final_output = output_text.split(\"<|assistant|>\")[-1]\n",
        "          final_output = final_output.replace(\"<s>\", \"\").replace(\":\", \"\")\n",
        "          if selected_task == 'Chat BOT':\n",
        "              final_output = output_text.split(\"<|assistant|>\")[-1]\n",
        "              final_output = final_output.replace(\"<s>\", \"\").replace(\":\", \"\")\n",
        "              bot_list.append(final_output)\n",
        "          else:\n",
        "              final_output = output_text.split(\"###Response\")[1]\n",
        "              final_output = final_output.replace(\"<s>\", \"\").replace(\":\", \"\")\n",
        "          return final_output\n",
        "\n",
        "  with gr.Blocks() as demo:\n",
        "      gr.Markdown(\"## Chat with your Buddy\")\n",
        "      task_dropdown = gr.Dropdown(model_prompts, label=\"Select Task\", allow_custom_value=False)\n",
        "      task_dropdown.value = model_prompts[0]\n",
        "\n",
        "      # Define additional inputs\n",
        "      custom_instruct_textbox = gr.Textbox(default=\"\", label=\"Custom instructions\", visible=task_dropdown.value == \"Custom instructions - Chat BOT\")\n",
        "      # https://discuss.huggingface.co/t/clear-chat-interface/49866/6\n",
        "      bot = gr.Chatbot(render=False)\n",
        "      # Create a chat interface with live updates\n",
        "      # Additional fields\n",
        "      top_p_slider = gr.Slider(0.1, 1.0, value=0.5, label=\"Top P\")\n",
        "      temperature_slider = gr.Slider(0.1, 1.0, value=0.8, label=\"Temperature\")\n",
        "      max_new_tokens_textbox = gr.Textbox(\"50\", label=\"Max New Tokens\", type=\"text\")\n",
        "\n",
        "\n",
        "      chat_interface = gr.ChatInterface(\n",
        "          generate,\n",
        "          chatbot=bot,\n",
        "          additional_inputs=[task_dropdown, custom_instruct_textbox, top_p_slider, temperature_slider, max_new_tokens_textbox],\n",
        "          autofocus=True  # Enable live updates\n",
        "      )\n",
        "\n",
        "      # Set initial visibility based on the default selected task\n",
        "      def update_visibility(selected_task):\n",
        "          prompt_list = []\n",
        "          bot_list = []\n",
        "          if selected_task == \"Custom instructions - Chat BOT\":\n",
        "              return gr.Textbox(label=\"Custom instructions\", visible=True, default=\"\")\n",
        "          else:\n",
        "              return gr.Textbox(visible=False)\n",
        "\n",
        "      # update_visibility()  # Call the function to set the initial visibility\n",
        "      task_dropdown.change(update_visibility, task_dropdown, [custom_instruct_textbox])\n",
        "      task_dropdown.change(reset, task_dropdown, outputs=[bot, chat_interface.chatbot_state])\n",
        "\n",
        "      demo.queue(concurrency_count=concurrency_count).launch(share=share)"
      ],
      "metadata": {
        "id": "ibCaUFueCNv1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_gradio_app(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "hmWr-MwKLRA-",
        "outputId": "b3c97c47-7ad7-4e17-b122-465a0b100fc8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-76d0d77aee1e>:86: GradioUnusedKwargWarning: You have unused kwarg parameters in Textbox, please remove them: {'default': ''}\n",
            "  custom_instruct_textbox = gr.Textbox(default=\"\", label=\"Custom instructions\", visible=task_dropdown.value == \"Custom instructions - Chat BOT\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://c8a2fba43aba09412a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c8a2fba43aba09412a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BukejVphLQ-L"
      },
      "execution_count": 99,
      "outputs": []
    }
  ]
}